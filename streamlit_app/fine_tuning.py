import streamlit as st
import pandas as pd
import pickle
import numpy as np
import plotly.graph_objects as go
from custom_functions import plot_auc, plot_f1_score, plot_loss_curve , plot_precision_curve

with open("models\history_DenseNet201.pkl", "rb") as file1:
    history_densenet = pickle.load(file1)
with open("models\history_VGG16.pkl", "rb") as file2:
    history_vgg = pickle.load(file2)

def show_fine_tuning():
    # Style des onglets
    st.markdown("""
        <style>
            .stTabs [data-baseweb="tab-list"] {
                display: flex;
                gap: 10px;
            }

            .stTabs [data-baseweb="tab"] {
                padding: 10px 15px;
                border: 1px solid transparent;
                border-radius: 5px 5px 0 0;
                background-color: transparent;
                cursor: pointer;
                transition: all 0.3s ease;
            }

            .stTabs [data-baseweb="tab"]:hover {
                background-color: #8f8d9b;
            }

            .stTabs [aria-selected="true"] {
                background-color:  #57546a;
                border-color: #ccc;
                border-bottom-color: transparent;
            }
        </style>""", unsafe_allow_html = True)

    tab0, tab1, tab2, tab3, tab4 = st.tabs(["üìö Rappels Deep Learning", "üõ†Ô∏è Preprocessing", "üìà Mod√©lisation", "üíª Mod√®les test√©s", "ü§ñ Mod√®les finaux"])
        
    #---------------------------------------------------------------------
    # Les deux fonctions suivantes pour centrer les images dans les pages
    # fonction qui coverti une image en foramt bytes
    def img_to_bytes(img_path):
        import base64
        from pathlib import Path
        img_bytes = Path(img_path).read_bytes()
        encoded = base64.b64encode(img_bytes).decode()
        return encoded
    # fonction qui coverti l'image encoder en html
    def img_to_html(img_path):
        img_html = "<img src='data:image/png;base64,{}' class='img-fluid'>".format(img_to_bytes(img_path)
        )
        return img_html
    #---------------------------------------------------------------------   
 

    ### Onglet 0 : Pr√©sentation d'un CNN
    with tab0:
        st.header("Deep Learning & CNN")
        st.write("#### 1. R√©seaux de neurones artificiels")
        st.markdown('''
            Un r√©seau de neurones est un ensemble de couches constitu√©s de **Perceptrons**. Ce entit√© de base cherche √† **imiter le fonctionnement d'un neurones biologique** gr√¢ce √† des concepts math√©matiques notamment le produits scalaires.\n
            Un Perceptron effectue des calculs pour d√©tecter des caract√©ristiques ou des tendances dans les donn√©es d‚Äôentr√©e.\n
            Un r√©seau neuronal *'**Feed-Forward**'* est constitu√© de plusieurs perceptron √† couches multiples.
        ''')
        
        #st.image(r".\images\neurone-biologique-et-artificiel.png", caption='Un neurone biologique vs un Perceptron (neurone artificiel)')
        # chemin du fichier de l'image
        image_path = r".\images\neurone-biologique-et-artificiel.png"
        # afficher l'image centr√©e avec markdown
        st.markdown("<p style='text-align: center; color: grey;'>" + img_to_html(image_path) + "</p>", unsafe_allow_html=True)
        # La l√©gende de l'image
        st.markdown("<div style='text-align: center; color: grey;'>Un neurone biologique vs un Perceptron (neurone artificiel)</div>", unsafe_allow_html=True)
        
        # S√©parateur ligne
        st.write("___")
        
        st.write("#### 2. Convolutional Neural Network (CNN)")
        
        st.markdown('''
        Les r√©seaux de neurones convolutifs d√©signent une sous-cat√©gorie de r√©seaux de neurones : ils pr√©sentent donc toutes les m√™mes caract√©ristiques d'un r√©seau de neurones. Cependant, les CNN sont sp√©cialement con√ßus pour traiter des images en entr√©e.\n
        Leur architecture est alors plus sp√©cifique : elle est compos√©e de deux blocs principaux: un extracteur de caract√©ristiques ou partie convolutive *'**features extraction bloc**'*, et un bloc pour la classification.\n
        
        La partie convolutive est constitu√© des couches suivantes:
        
        - Convolution : en utilisant des **filtres** et le **produit de convolution**, les caract√©ristiques de l'image d'entr√©e sont extraites.
        - Pooling : m√©thode de sous √©chantillonnage, l'objectif est de sous-√©chantillonner l'entr√©e en r√©duisant sa dimension. L'int√©r√™t est la r√©duction du co√ªt de calcul **en r√©duisant le nombre de param√®tres √† apprendre**. les deux m√©thodes les plus utilis√©es sont: le **Max-Pooling** (valeur maximum) et l'**Average Pooling** (valeur moyenne).
        
        ''')
        
        #st.image(r".\images\layers_CNN.png", caption="Architecture d'un r√©seau de neurones convolutifs CNN")
        image_path = r".\images\layers_CNN.png"
        # afficher l'image centr√©e avec markdown
        st.markdown("<p style='text-align: center; color: grey;'>" + img_to_html(image_path) + "</p>", unsafe_allow_html=True)
        # La l√©gende de l'image
        st.markdown("<div style='text-align: center; color: grey;'>Architecture d'un r√©seau de neurones convolutifs CNN</div>", unsafe_allow_html=True)
        
        # D√©monstration avec l'application de reconnaissance de chiffres 
        st.button("Reset", type="primary")
        if st.button('DEMO'):
                st.write("##### D√©monstration en direct : fonctionnement d'un CNN")
                st.link_button("DEMO Chiffre √©crit √† la main", "https://adamharley.com/nn_vis/cnn/3d.html")


    ### Premier onglet
    with tab1:
        st.header("Preprocessing des images")
        
        st.write("#### 1. Metadata des images")
        st.markdown('''
            Une √©tape tr√®s importante de notre projet est l'attention port√©e au traitement des images d'entr√©e. Nous avons pu voir pr√©c√©demment que les images poss√®dent pour certaines, des dimensions et/ou un nombre de canaux diff√©rents. Il est important d'homog√©n√©iser l'ensemble des param√®tres de nos images pour assurer une bonne performance de nos mod√®les, et surtout, des r√©sultats comparables. Les √©l√©ments en question sont :
            - Une dimension homog√®ne et carr√©e, par d√©faut 299x299 pixels.
            - Un nombre de trois canaux de couleur.
            - Une normalisation de la valeur des pixels.\n
            Une fonction `preproc_img()` est con√ßue pour simplifier ces √©tapes, am√©liorer la reproductibilit√© et faciliter les ajustements. Elle retourne automatiquement les **ensembles d'entra√Ænement et de test**.
        ''')
        
        # S√©parateur ligne
        st.write("___")
        
        st.write("#### 2. Fonctions de pre-processing")
        # Style CSS pour listes √† puces internes
        st.markdown('''
        <style>
        [data-testid="stMarkdownContainer"] ul{
            list-style-position: inside;
        }
        </style>
        ''', unsafe_allow_html = True)

        # D√©finir le code comme une cha√Æne de caract√®res longue
        code = """
                def preproc_img(df_images, df_masks, n_img, normalize, files_path, resolution, with_masks):
                    np.random.seed(42)
                    # Gestion des erreurs
                    if resolution[2] != 1 and resolution[2] != 3:
                        return print("Le nombre de canaux doit √™tre de 1 (en nuances de gris) ou de 3 (en couleur)")

                    if resolution[0] != resolution[1]:
                        return print("La largeur de l'image doit √™tre la m√™me que sa hauteur.")
                    
                    if normalize != 'imagenet' and normalize != 'simple':
                        print(Attention : aucune normalisation n'a √©t√© appliqu√©e. Utilisez 'imagenet' pour une normalisation standardis√©e selon le mode op√©ratoire du set ImageNet ou 'simple' pour simplement normaliser la valeur des canaux entre 0 et 1.")

                    df_images_selected_list = []
                    for label, group in df_images.groupby('LABEL'):
                        n_samples = min(len(group), n_img)
                        df_images_selected_list.append(group.sample(n=n_samples, replace=False))
                    df_images_selected = pd.concat(df_images_selected_list)

                    images = []  # Initialiser une liste pour stocker les images pr√©trait√©es
                    df_masks_selected = df_masks[df_masks['FILE_NAME'].isin(df_images_selected['FILE_NAME'])] if with_masks else None

                    for i in range(len(df_images_selected)):
                        img_path = df_images_selected[files_path].iloc[i]
                        mask_path = df_masks_selected[files_path].iloc[i] if with_masks else None

                        img = Image.open(img_path)  # Charger l'image avec PIL
                        img = img.convert("L") if resolution[2] == 1 else img.convert("RGB")

                        img_resized = img.resize((resolution[0], resolution[1]))

                        # Normalisation des valeurs des pixels
                        if normalize == 'imagenet':
                            img_normalized = np.array(img_resized) / 255.0
                            img_normalized -= np.array([0.485, 0.456, 0.406]) if resolution[2] == 3 else np.mean([0.485, 0.456, 0.406])
                            img_normalized /= np.array([0.229, 0.224, 0.225]) if resolution[2] == 3 else np.mean([0.229, 0.224, 0.225])
                        elif normalize == 'simple':
                            img_normalized = img_resized / 255
                        else:
                            img_normalized = img_resized

                        images.append(img_normalized)

                    data = np.array(images).reshape(-1, resolution[0], resolution[1], resolution[2])
                    target = df_images_selected['LABEL']
                    return data, target
                """

        with st.expander("Voir le code de la fonction preproc_img()"):
            st.code(code, language = 'python')
        
        
        st.markdown('''Le processus de pr√©traitement des donn√©es consiste √† uniformiser les donn√©es en les important via `OpenCV` avec `cv2.IMREAD_GRAYSCALE` et en les convertissant en uint8 pour √©conomiser de la m√©moire. 
                       Les images peuvent √™tre redimensionn√©es √† la r√©solution de notre choix, stock√©es sous forme d'arrays numpy. 
                       Une normalisation de l'intensit√© des pixels peut √™tre appliqu√©e selon les besoins et les attentes des mod√®les, et des m√©thodes d'√©quilibrage des classes comme l'undersampling ou l'oversampling peuvent √™tre envisag√©es en raison de diff√©rences significatives dans leur r√©partition. 
                       Les premiers masques sont utilis√©s pour limiter la surface aux informations utiles, avec la possibilit√© de cr√©er de nouveaux masques.
                    ''')
        
        # S√©parateur ligne
        st.write("___")
        
        st.write("#### 3. Encodage des labels")
        
        st.markdown(''' Derni√®re √©tape apr√®s nos images propres et normalis√©es, il est n√©cessaire de transformer nos labels multiclasses en entiers afin d'assurer la compatibilit√© avec une les mod√®les de classificiation.
                        Cette √©tape n√©cessite seulement un traitement par **One Hot Encoding** gr√¢ce √† `LabelEncoder()`.
                    ''')
        
        data = {
            'Classes': ['COVID', 'Lung_Opacity', 'Normal', 'Viral Pneumonia'],
            'Num√©ros correspondants': [0, 1, 2, 3]
        }
        df = pd.DataFrame(data)

        # Convertir le dataframe en HTML avec les styles CSS
        html_table = df.to_html(index=False, justify='center', classes='styled-table')

        # Afficher le HTML dans Streamlit avec la largeur calcul√©e
        st.markdown(f"<div style='border: 1px solid white; border-radius: 5px; padding: 10px; background-color: #343434; line-height: 1; width: 350px; margin: 0 auto;'>{html_table}</div>", unsafe_allow_html=True)
    
    
    
    ### Deuxi√®me onglet
    with tab2:
        st.header("D√©marche de mod√©lisation")
        st.markdown("Nous nous sommes mis d'accord pour commencer par un mod√®le basique, en l'occurrence **LeNet5**, eafin de prendre en main la mod√©lisation en Deep Learning. Ensuite, travailler avec des mod√®les plus complexes qui sont disponibles dans **Keras Applications**, nous avons fait du **transfert learning** √† partir de ces mod√®les-l√†, en r√©entrainant les derni√®res couches sur notre base de donn√©es. Enfin, avec le module **Keras Tuner** nous avons pu ajuster plus finement nos mod√®les.  ")
        
        # LeNet5 
        st.write("#### 1. LeNet5")
        st.markdown(''' LeNet est une structure de r√©seau neuronal convolutif propos√©e par LeCun et al. en 1998. En g√©n√©ral, LeNet fait r√©f√©rence √† LeNet-5 et est un r√©seau neuronal convolutif simple. Les r√©seaux neuronaux convolutifs sont une sorte de r√©seau neuronal feed-forward dont les neurones artificiels peuvent r√©pondre √† une partie des cellules environnantes dans la zone de couverture et donnent de bons r√©sultats dans le traitement d'images √† grande √©chelle. *Source: https://en.wikipedia.org/wiki/LeNet*. 
                    ''')        
        #st.image(r".\images\LeNet5_architecture.png", caption="Architecture du LeNet5")
        # chemin du fichier de l'image
        image_path = r".\images\LeNet5_architecture.png"
        # afficher l'image centr√©e avec markdown
        st.markdown("<p style='text-align: center; color: grey;'>" + img_to_html(image_path) + "</p>", unsafe_allow_html=True)
        # La l√©gende de l'image
        st.markdown("<div style='text-align: center; color: grey;'>Architecture du LeNet5</div>", unsafe_allow_html=True)
        
        
        
        
        st.write("##### Etudes param√©triques: nombre d'images et nombre d'Epochs")
        
        st.markdown(''' L'efficacit√© et la simplicit√©, du mod√®le LeNet5, nous ont permis de r√©aliser des √©tudes param√©triques assez rapidement afin de le nombre d'images et d'√©poques √† partir desquels les performances du mod√®le n'√©voluent plus. Ceci nous a permis d'√©conomiser en temps et co√ªt de calcul par la suite en utilisant des mod√®les plus complexes.  
                    ''') 
        
        col1, col2 = st.columns([1, 1])
        
        with col1:
            st.image(r".\images\LeNet-5_benchmark_n_img.png", caption="Courbe d‚Äôapprentissage du mod√®le LeNet-5 en fonction du nombre d‚Äôimages utilis√©es")
            
            
        with col2:
            st.image(r".\images\LeNet-5_benchmark_epochs.png", caption="Courbe d‚Äôapprentissage du mod√®le LeNet-5 en fonction du nombre d‚Äô√©poques")            
               
        st.markdown('''
        Par la suite, nous entrainons les mod√®les avec les param√®tres suivants : 
        - 900 images par classe.
        - 20 epochs.
        
        ''')
    
        # S√©parateur ligne
        st.write("___")
              
        # 2. Keras Tuner
        st.write("#### 2. Keras Tuner")  
        st.markdown('''
        Keras Tuner est un module qui permet de r√©aliser une √©tude d‚Äôoptimisation des hyperparam√®tres afin de trouver les meilleures combinaisons de param√®tres, permettant d‚Äôajuster un peu plus finement le mod√®le (O‚ÄôMalley et al., 2019).\n

        Il existe plusieurs fonctions int√©ressantes pour la recherche de param√®tres optimaux pour un ajustement plus fin des mod√®les. RandomSearch() est tr√®s pratique pour chercher de mani√®re al√©atoire ces hyperparam√®tres optimaux,  elle prend en argument le mod√®le, la m√©trique √† maximiser, les param√®tres √† faire varier, etc.
        
        ''')

        # D√©finir le code comme une cha√Æne de caract√®res longue
        code = """
            # 1. D√©finir une fonction qui construit le mod√®le avec les HP
            def build_model(hp):
                model = keras.Sequential()
                model.add(layers.Flatten())
                # Tune the number of layers.
                for i in range(hp.Int("num_layers", 1, 3)):
                    model.add(
                        layers.Dense(
                            # Tune number of units separately.
                            units=hp.Int(f"units_{i}", min_value=32, max_value=512, step=32),
                            activation=hp.Choice("activation", ["relu", "tanh"]),
                        )
                    )
                if hp.Boolean("dropout"):
                    model.add(layers.Dropout(rate=0.25))
                model.add(layers.Dense(10, activation="softmax"))
                learning_rate = hp.Float("lr", min_value=1e-4, max_value=1e-2, sampling="log")
                model.compile(
                    optimizer=keras.optimizers.Adam(learning_rate=learning_rate),
                    loss="categorical_crossentropy",
                    metrics=["accuracy"],
                )
                return model

            build_model(keras_tuner.HyperParameters())
            
            
            # 2. RandomSearch pour chercher les meilleurs combinaison d'hyperparam√®tres
            tuner = keras_tuner.RandomSearch(
                hypermodel=build_model,
                objective="val_accuracy",
                max_trials=3,
                executions_per_trial=2,
                overwrite=True,
                directory="my_dir",
                project_name="helloworld",
            )
            """
        with st.expander("Voir le code de KerasTuner"):
            st.code(code, language = 'python')
            

        # s√©parer les sections avec une ligne
        st.write("___")
        
        # 3. Transfer Learning
        st.write("#### 3. Transfer Learning")  
        st.markdown('''
        Le transfer learning est une technique en apprentissage automatique o√π un mod√®le pr√©-entra√Æn√© sur une t√¢che est r√©utilis√© comme point de d√©part pour r√©soudre une autre t√¢che similaire. 
        Plut√¥t que de construire un nouveau mod√®le √† partir de z√©ro, on exploite les connaissances et les repr√©sentations d√©j√† apprises (les poids), ce qui permet d'am√©liorer l'apprentissage sur des ensembles de donn√©es plus petits ou diff√©rents. 
        ''')       
        
        # Tableau qui r√©sume les mod√®les choisis pour le Transfet Learning
        data = {
            'Mod√®le': ['InceptionResNet', 'ResNet', 'DenseNet', 'VGG', 'ConvNext', 'EfficientNet'],
            'Versions': ['InceptionResNetV2', 'ResNet121V2', 'DenseNet201', 'VGG16, VGG19', 'ConvNextBase, ConvNextTiny', 'EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3, EfficientNetB4, EfficientNetB5, EfficientNetB6']
        }
        df = pd.DataFrame(data)

        # Convertir le dataframe en HTML avec les styles CSS
        html_table = df.to_html(index=False, justify='center', classes='styled-table')

        # Afficher le HTML dans Streamlit avec la largeur calcul√©e
        st.markdown(f"<div style='border: 1px solid white; border-radius: 5px; padding: 10px; background-color: #343434; line-height: 1; width: 350px; margin: 0 auto;'>{html_table}</div>", unsafe_allow_html=True)           

    ### Troisi√®me onglet
    with tab3:
            Slider = st.select_slider(" ", options = ["Transfer learning" , "Fine Tuning"])

            categorie = {"Transfer learning" :["Mod√®les test√©s","InceptionResNetV2","ResNet121V2","DenseNet201","VGG16", "VGG19","ConvNextTiny","ConvNextBase","EfficientNet B4"],
                        "Fine Tuning" : ["EfficientNet", "ResNet", "VGG16_ft" ,"DenseNet"]}

            Choice_cr = st.selectbox("Navigation",
                                    options = categorie[Slider])
            
            csv_path_cr = {"Mod√®les test√©s" :r"data\df test model.csv",
                        "InceptionResNetV2" :r"data\df InceptionRes.csv",
                        "ResNet121V2" : r"data\df Res.csv",
                        "DenseNet201": r"data\df densenet.csv",
                        "VGG16" : r"data\df VGG16.csv", 
                        "VGG19" : r"data\df VGG19.csv",
                        "ConvNextTiny" : r"data\df Convtiny.csv",
                        "ConvNextBase" : r"data\df Convbase.csv",
                        "EfficientNet B4" :r"data\df efficient.csv",
                        "EfficientNet" :r"data\df efficientnet finetuned.csv",
                        "ResNet" :r"data\df resnet finetuned.csv",
                        "VGG16_ft" :r"data\df VGG16_finetuned.csv",
                        "DenseNet" :r"data\df densenet_finetuned.csv"}
            
            comm_dico = {"Mod√®les test√©s" :""" Voici un r√©capitulatif des mod√®les que nous avons test√© dans le cadre du transfer learning. """,
                        "InceptionResNetV2" :""" Le mod√®le a une capacit√© variable √† distinguer les diff√©rentes classes de radiographies. La classe Viral Pneumonia pr√©sente d'excellents scores de pr√©cision, de rappel et de F1, indiquant une identification quasi parfaite, tandis que la classe Normal a montr√© des difficult√©s plus marqu√©es, avec les scores les plus bas pour ces m√™mes m√©triques. Le score F1, qui √©quilibre la pr√©cision et le rappel, sugg√®re que le mod√®le est plus apte √† identifier correctement les classes COVID et Viral Pneumonia, __mais qu'il pourrait b√©n√©ficier d'un r√©√©quilibrage ou d'un ajustement dans la classification des classes Lung_Opacity et Normal.__ """ ,
                        "ResNet121V2" :""" Le mod√®le a une certaine tendance √† confondre la classe COVID avec les classes Lung_Opacity et Normal, comme en t√©moignent les 11 erreurs dans chaque cas. N√©anmoins, la classe Viral Pneumonia est interpr√©t√©e avec une grande pr√©cision, indiquant que les caract√©ristiques distinctives de cette classe sont bien captur√©es par le mod√®le. Les m√©triques par classe montrent que la classe 3 se distingue avec une pr√©cision et un rappel exceptionnels proches de 0.98, menant √† un score F1 similaire, qui est une mesure robuste de la pr√©cision globale. Les classes COVID, Lung_Opacity, et Normal pr√©sentent des scores F1 l√©g√®rement plus bas, mais toujours respectables, bien que ces classes pourraient b√©n√©ficier d'un r√©ajustement du mod√®le pour am√©liorer la distinction entre elles. __La pr√©cision globale du mod√®le √† 0.88 est solide, mais l'objectif serait de viser une am√©lioration dans la classification fine entre les classes similaires.__""" ,
                        "DenseNet201":"""  Les erreurs de classification les plus courantes semblent se produire entre les classes Lung_Opacity et Normal, sous-entendant des similarit√©s entre les caract√©ristiques des radiographies que le mod√®le confond certainement. Selon le tableau de m√©triques, le mod√®le a une excellente pr√©cision pour la classe COVID et des scores exceptionnels de rappel et de F1 pour la classe Viral Pneumonia, indiquant une classification presque parfaite pour ces cat√©gories. Les classes Lung_Opacity et Normal ont des scores F1 l√©g√®rement inf√©rieurs mais comparables. __Tout ceci indique une bonne performance de classification qui reste uniforme entre ces cat√©gories.__""" ,
                        "VGG16" : """ Le mod√®le parvient √† tr√®s bien classer les radiographies des classes Viral (Viral pneumonia) et COVID. √âgalement, m√™me si les r√©sultats restent bons, le mod√®le commet plus d'erreurs de classification entre les cat√©gories Normal et Lung (Lung_opacity). __Sans ajustement particulier, ce mod√®le semble d√©j√† prometteur quant √† ses capacit√©s √† classifier nos radiographies correctement.__""", 
                        "VGG19" : """ Les r√©sultats obtenus semblent √©galement tr√®s bons et superposables  √† ceux que nous avons obtenus pour que pour VGG16. __Cependant ce mod√®le √©tant un peu plus profond, il demande des ressources computationnelles plus importantes sans que cela ne se r√©percute de fa√ßon √©vidente sur ses performances.__ """,
                        "ConvNextTiny" : """ Avec ce mod√®le il appara√Æt que la classification est significativement meilleure pour la cat√©gorie  Viral (Viral pneumonia) que pour les autres. Ceci donne un score global en de√ß√† de ce que nous avons pu observer sur d‚Äôautres mod√®les dans les m√™mes conditions de test. Les courbes d‚Äôapprentissage sugg√®rent que le mod√®le pourrait b√©n√©ficier d‚Äôun nombre d'√©poques sup√©rieur pour continuer √† s‚Äôam√©liorer. """,
                        "ConvNextBase" : """ La classe Viral pneumonia reste toujours la mieux d√©tect√©e, suivie de la classe COVID. Les r√©sultats obtenus ici sont donc comparables √† ceux obtenus avec le mod√®le ConvNeXtTiny. Encore une fois le mod√®le semble pouvoir b√©n√©ficier d‚Äôun allongement de la dur√©e d'entra√Ænement. Cependant il est √† noter que ce mod√®le peut se montrer gourmand en termes de ressource computationnelle, __une √©poque de ConvNeXtBase pouvant prendre entre deux et trois fois plus de temps que le mod√®le ConvNeXtTiny sans montrer une diff√©rence flagrante de performance.__""",
                        "EfficientNet B4" :"""La pr√©cision du mod√®le chute √† 0.88 sur l‚Äôensemble test. La d√©tection de la classe COVID n'est pas au niveau de ce que l‚Äôon esp√©rait avec une pr√©cision de 0.91. __Globalement, le mod√®le avec ce param√©trage donne de bons r√©sultats.__ Dans la section suivante, nous allons essayer un ajustement plus fin pour avoir de  meilleures performances avec ce mod√®le. """,
                        "EfficientNet" :""" __Avec une pr√©cision globale de 0.94, c‚Äôest le meilleur mod√®le que nous avons eu pour cette partie concernant la famille de mod√®les EfficientNet.__ De plus, le mod√®le semble bien plus performant concernant la classe qui nous int√©resse ici (classe COVID), avec une reconnaissance des radiographies COVID √† 0.98 avec pr√©cision. Pour la suite de nos travaux, le meilleur mod√®le sera adopt√© et utilis√© pour l‚Äôinterpr√©tabilit√© et la suite de cette √©tude.""",
                        "ResNet" :""" Bien que performant, le mod√®le tend √† √™tre frein√© dans ses performances par la classe Lung_Opacity, dans laquelle il classe des poumons sains et vice-versa. Quelques poumons sains sont aussi incorrectement class√©s en Viral Pneumonia. __Ce mod√®le est donc performant, et supprimer la classe Lung_Opacity b√©n√©ficierait certainement beaucoup au InceptionResNetV2.__""",
                        "VGG16_ft" :""" Le mod√®le a donc √©t√© entra√Æn√© avec ces param√®tres ce qui nous permet d‚Äôam√©liorer encore l‚Äôefficacit√© du mod√®le par rapport √† ce que nous avions obtenu sans finetuning. Les classes les mieux pr√©dites sont COVID et Viral (Viral pneumonia), suivies par les classes Lung Opacity et Normal. De fa√ßon int√©ressante , toutes nos m√©triques sont au-dessus de 90% et suite √† l'entra√Ænement du mod√®le avec les meilleurs param√®tres nous obtenons une accuracy globale de 95%. __Ce mod√®le semble donc capable de fournir des r√©sultats plus qu‚Äôacceptables tout en ayant un co√ªt computationnel tr√®s contenu.__""",
                        "DenseNet" :""" Le rapport de classification montre des valeurs √©lev√©es pour la pr√©cision, le rappel et le F1 Score pour chaque classe ce qui indique que le mod√®le est particuli√®rement performant dans la distinction entre les diff√©rentes conditions. A noter cependant qu‚Äôil performe tout particuli√®rement dans la distinction de la classe COVID et de la classe Viral Pneumonia mais est un peu moins efficace dans la d√©tection des classes Normal et Lung_Opacity. Pour le COVID, le mod√®le a tr√®s bien perform√©, avec seulement 3 faux positifs et faux n√©gatifs. Les r√©sultats pour les autres conditions sont √©galement bons, mais on note quelques erreurs, par exemple, 23 cas de Lung_Opacity ont √©t√© confondus avec la classe Normal. __N√©anmoins, ces erreurs semblent √™tre faibles en comparaison avec le nombre total de pr√©dictions correctes.__"""}


            df = pd.read_csv(csv_path_cr[Choice_cr])
            df= df.fillna("")

            # Convertir le dataframe en HTML avec les styles CSS
            html_table = df.to_html(index=False, justify='center', classes='styled-table')

            # D√©finir le style CSS pour centrer l'affichage du DataFrame et le fond
            css_style = """
            <style>
                .background-div {
                    max-width: fit-content; /* Largeur adapt√©e au contenu */
                    margin: 0 auto; /* Centrer horizontalement */
                    padding: 10px;
                    background-color: #343434;
                    border-radius: 5px;
                }
                .inner-div {
                    text-align: center; /* Centrer le contenu */
                }
            </style>
            """

            # Ajouter le style CSS √† la balise div
            styled_html_table = f"<div class='background-div'><div class='inner-div'>{html_table}</div></div>"

            col1, col2 = st.columns(2)

    with col1:
        st.markdown(css_style, unsafe_allow_html=True)
        st.markdown(styled_html_table, unsafe_allow_html=True)
    
    with col2:
        st.markdown(comm_dico[Choice_cr])


    ### Quatri√®me onglet
    with tab4:

        model_f = st.selectbox ('Meilleurs mod√®les', options = ["VGG16" , "DenseNet"] ) 

        path_pickle = {"VGG16" : r"models\history_VGG16.pkl",
                    "DenseNet" : r"models\history_DenseNet201.pkl"}
        
        best_hp = {"VGG16" : """ 
                   - Derni√®re couche dense : 1024 neurones
                   - Dropout : 0
                   - Learningrate : 10e-4 """,
                   "DenseNet" : """ 
                    - Derni√®re couche dense : 256 neurones (Regularisation L2 : 0.01)
                    - Dropout : 0.4,
                    - Learning rate : 10e-4 """}
        
        with open(path_pickle[model_f], 'rb') as fichier:
        # Charger les donn√©es √† partir du fichier
            history = pickle.load(fichier)
        
        Col1 , Col2 = st.columns(2)

        with Col1:
            plot_loss_curve(history)
            plot_auc(history)
        
        with Col2:
            plot_precision_curve(history)
            plot_f1_score(history)

        st.markdown(best_hp[model_f])
